#########################
#
# Master parameters
#

[master]

# user name of the daemon process
uname = atlpan

# group name of the daemon process
gname = zp

# logger name
loggername = harvester

# harvester id - unique id as registered also in panda server
harvester_id = cern_cloud



##########################
#
# Database parameters
#

[db]

# filename
database_filename = FIXME

# verbose
verbose = False
#verbose = True

# number of database connections in each process
nConnections = 10

# database engine : sqlite or mariadb
#engine = sqlite
engine = mariadb

# user name
user = harvester

# password
password = xxxxxxx 

# schema
#schema = HARVESTER
schema = harvester




##########################
#
# Panda Connection parameters
#

[pandacon]

# number of connections
nConnections = 5

# timeout
timeout = 180

# CA file
ca_cert = /etc/pki/tls/certs/CERN-bundle.pem

# certificate
cert_file = /data/atlpan/proxy/x509up_u25606_pilot

# key
key_file = /data/atlpan/proxy/x509up_u25606_pilot

# base URL via http
pandaURL = http://pandaserver.cern.ch:25080/server/panda

# base URL via https
pandaURLSSL = https://pandaserver.cern.ch:25443/server/panda

# base URL via proxy
pandaURLProxy = http://pandaserver.cern.ch:25080/server/panda

# base URL for write access to log cache server
pandaCacheURL_W = https://aipanda011.cern.ch:25443/server/panda

# base URL for read access to log cache server
pandaCacheURL_R = https://aipanda011.cern.ch:25443/cache

# verbose
verbose = False
#verbose = True




##########################
#
# Queue Config parameters
#

[qconf]

# config file
configFile = panda_queueconfig.json

# queue list : one queue name following a whitespace per line
queueList =
 ALL

# module and class names to resolve queue names to panda queue names
resolverModule = pandaharvester.harvestermisc.info_utils
resolverClass = PandaQueuesDict

# enable auto-blacklisting of resolver which returns status='offline' to blacklist the queue
autoBlacklist = True


##########################
#
# FIFO parameters
#

[fifo]
# module and class names to provide the fifo queue
fifoModule = pandaharvester.harvesterfifo.sqlite_fifo
fifoClass = SqliteFifo

# database filename for sqlite.
# must be different from main Harvester DB and other fifo DBs if using sqlite
# different db filenames for fifos of of different agents
# placeholder $(AGENT) should be used in filename; it will then be changed to the agent name
database_filename = /dev/shm/$(AGENT)_fifo.db




##########################
#
# Command manager parameters
#
[commandmanager]

# bulk size for panda server interactions
commands_bulk_size = 20

# sleep interval in sec
sleepTime = 5




##########################
#
# Job Fetcher parameters
#

[jobfetcher]

# number of threads
#nThreads = 3
nThreads = 1

# number of queues to fetch jobs in one cycle
#nQueues = 5
nQueues = 30

# max number of jobs per queue in one cycle
maxJobs = 500

# lookup interval in sec
lookupTime = 60

# sleep interval in sec
sleepTime = 60




##########################
#
# Propagator parameters
#

[propagator]

# number of threads
#nThreads = 3
nThreads = 2

# max number of jobs tp update in one cycle
maxJobs = 500

# number of jobs in bulk update
nJobsInBulk = 50

# max number of workers tp update in one cycle
maxWorkers = 500

# number of workers in bulk update
nWorkersInBulk = 50

# lock interval in sec
lockInterval = 180

# update interval in sec
#updateInterval = 1800
updateInterval = 120

# sleep interval in sec
sleepTime = 20




##########################
#
# Preparator parameters
#

[preparator]

# number of threads
#nThreads = 3
nThreads = 1

# max number of jobs to check in one cycle
maxJobsToCheck = 100

# max number of jobs to trigger in one cycle
maxJobsToTrigger = 100

# lock interval in sec
lockInterval = 180

# check interval in sec
checkInterval = 180

# trigger interval in sec
triggerInterval = 180

# sleep interval in sec
sleepTime = 60




##########################
#
# Submitter parameters
#

[submitter]

# number of threads
#nThreads = 3
nThreads = 1

# max number of wokers per queue to try in one cycle
#maxWorkers = 200

# max number of queues to try in one cycle
#nQueues = 3
nQueues = 30

# interval for queue lookup
lookupTime = 60

# lock interval in sec
#lockInterval = 180
lockInterval = 900

# check interval in sec
checkInterval = 600

# sleep interval in sec
sleepTime = 60




##########################
#
# Monitor parameters
#

[monitor]

# number of threads
#nThreads = 3
nThreads = 2 

# max number of wokers per queue to try in one cycle
maxWorkers = 750

# lock interval in sec
lockInterval = 300

# check interval in sec
checkInterval = 900

# sleep interval in sec
#sleepTime = 30
sleepTime = 1200

# whether to use fifo
fifoEnable = True

# sleep interval in millisecond using fifo
fifoSleepTimeMilli = 1500

# check interval in fifo in sec
fifoCheckInterval = 300

# max number of workers to be populated into the fifo queue
fifoMaxWorkersToPopulate = 10000

# max number of workers in a chunk to enqueue
fifoMaxWorkersPerChunk = 500

# interval of force enqueue in sec
fifoForceEnqueueInterval = 3600

# max interval in sec a post-processing worker can preempt in fifo
fifoMaxPreemptInterval = 60


##########################
#
# Credential Manager parameters
#
# Notes : This is an example to manage two credentials, one with production role and the other with pilot role. One credential data following a whitespace per line. Empty lines are not allowed, so that a dummy string like 'dummy' needs to be added if some parameters like voms are unnecessary.

[credmanager]

# module name
moduleName =
 pandaharvester.harvestercredmanager.no_voms_cred_manager
 pandaharvester.harvestercredmanager.no_voms_cred_manager

# class name
className =
 NoVomsCredManager
 NoVomsCredManager

# original certificate file to generate new short-lived certificate
certFile =
 /data/atlpan/proxy/atlpilo1RFC.plain
 /data/atlpan/proxy/atlpilo1RFC.plain

# the name of short-lived certificate
outCertFile =
 /data/atlpan/proxy/x509up_u25606_pilot
 /data/atlpan/proxy/x509up_u25606_prod

# voms
voms =
 atlas:/atlas/Role=pilot
 atlas:/atlas/Role=production

# sleep interval in sec
sleepTime = 1800




##########################
#
# Stager parameters
#

[stager]

# number of threads
#nThreads = 3
nThreads = 1

# max number of jobs to check in one cycle
maxJobsToCheck = 100

# max number of jobs to trigger in one cycle
maxJobsToTrigger = 100

# max number of jobs to zip in one cycle
maxJobsToZip = 100

# lock interval in sec
lockInterval = 180

# check interval in sec
checkInterval = 180

# trigger interval in sec
triggerInterval = 180

# sleep interval in sec
sleepTime = 60





##########################
#
# EventFeeder parameters
#

[eventfeeder]

# number of threads
#nThreads = 3
nThreads = 1

# max number of workers per queue to try in one cycle
maxWorkers = 500

# lock interval in sec
lockInterval = 180

# sleep interval in sec
sleepTime = 60





##########################
#
# Cacher parameters
#

[cacher]

# one data (main_key_name|sub_key_name|URL) following a white space per line
#
# Notes: This example is for five data. ddm_endpoints and panda_queues json files are retrieved using http.
#        It also caches proxy files which are renewed by Credential Manager. Access key for BNL object store
#        is retrieved from panda.
data =
 ddmendpoints_objectstores.json||http://atlas-agis-api.cern.ch/request/ddmendpoint/query/list/?json&state=ACTIVE&site_state=ACTIVE&preset=dict&json_pretty=1&type[]=OS_LOGS&type[]=OS_ES
 panda_queues.json||http://atlas-agis-api.cern.ch/request/pandaqueue/query/list/?json&preset=schedconf.all&vo_name=atlas

# proxy_pilot||file://path_to/FIXME_proxy_pilot
# proxy_production||file://path_to/FIXME_proxy_production
# BNL_key||panda_cache:BNL_ObjectStoreKey.pub&BNL_ObjectStoreKey

# refresh interval in minint
refreshInterval = 10

# sleep interval in sec
sleepTime = 60





##########################
#
# Payload interaction parameters
#

[payload_interaction]

# worker attributes
workerAttributesFile = worker_attributes.json

# job report
jobReportFile = jobReport.json

# event status dump file in json
eventStatusDumpJsonFile = event_status.dump.json

# event status dump file in xml
eventStatusDumpXmlFile = _event_status.dump

# job request
jobRequestFile = worker_requestjob.json

# job spec file
#jobSpecFile = HPCJobs.json
jobSpecFile = pandaJobData.out

# event request
eventRequestFile = worker_requestevents.json

# event ranges file
eventRangesFile = JobsEventRanges.json

# update events
updateEventsFile = worker_updateevents.json

# PFC for input files
xmlPoolCatalogFile = PoolFileCatalog_H.xml

# get PandaIDs
pandaIDsFile = worker_pandaids.json




##########################
#
# Front-end parameters
#

[frontend]

# port number for simple http frontend. For apache frontend port number is set in httpd.conf
portNumber = 25080

# number of threads
nThreads = 10

# verbose
#verbose = False
verbose = True

# type : simple or apache
type = apache

# file of secret used in token signature
secretFile = /data/atlpan/harvester_jwt.secret

# whether to verify token (of its signature, expiration, etc.) when decoding token
verifyToken = False




##########################
#
# Sweeper parameters
#

[sweeper]

# number of threads
#nThreads = 3
nThreads = 1

# max number of workers per queue to try in one cycle
maxWorkers = 500

# check interval in sec
checkInterval = 180

# sleep interval in sec
sleepTime = 60

# duration in hours to keep finished workers
keepFinished = 24

# duration in hours to keep failed workers
keepFailed = 72

# duration in hours to keep cancelled workers
#keepCancelled = 72
keepCancelled = 24

# duration in hours to keep missed workers
keepMissed = 4



##########################
#
# Watcher parameters
#

[watcher]

# action is taken when the last message is older than maxStalled sec. set 0 to disable the action
maxStalled = 300

# the number of messages to check interval
nMessages = 1000

# action is taken when it took more than maxDuration sec to generate nMessages messages. set 0 to disable the action
#maxDuration = 600
maxDuration = 0

# check interval in sec
checkInterval = 180

# sleep interval in sec
sleepTime = 600

# a comma-concatenated list of actions (email: to send alarms, kill: to be killed). or empty if no action
actions =

# name of env variable to keep pass-phrase
passphraseEnv = HARVESTER_WATCHER_PASSPHRASE

# hostname of SMTP server. note that parameters with the prefix of "mail" are required on
mailServer = localhost

# port of SMTP server
mailPort = 25

# use SSL for SMTP
mailUseSSL = False

# login user of SMTP server if any. leave it empty if SMTP doesn't need to logon
mailUser =

# login password of SMTP server if any. leave it empty if SMTP doesn't need to logon
mailPassword =

# email sender
mailFrom = example_from@example.com

# a comma-concatenated list of email recipients
mailTo = example_to_1@example.com,example_to_2@example.com

##########################
#
# Google cloud parameters
#

[googlecloud]

# zone where you are booting up your VMs and storage, e.g. us-east1-b, europe-west3-a
zone = us-east1-b
# project defined in the google compute account, where the activity will be billed
project = atlas-harvester-199512
# private service account json generated in the google cloud management console
# service_account_file = /root/gce/atlas-harvester-d75f556f6478.json
service_account_file = /usr/etc/panda/atlas-harvester-d75f556f6478.json
# file with the user data to send to CERN VM
user_data_file = /usr/etc/panda/google_user_data.txt
# image to use
image = https://www.googleapis.com/compute/v1/projects/atlas-harvester-199512/global/images/cernvm4-micro-3-0-6-bnl
# harvester frontend
harvester_frontend = https://aipanda170.cern.ch:25443/entry
